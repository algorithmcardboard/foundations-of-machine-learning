\documentclass{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Anirudhan J Rajagopalan - ajr619 - N18824115}

\begin{document}

\title{Foundations of Machine Learning --- Homework Assignment 1}
\date{October 11, 2015}
\author{Anirudhan J Rajagopalan\\ N18824115\\ ajr619}

\maketitle

\newpage

\section*{A. PAC Learning}
\subsection*{1}
\begin{description}
  \item[Algorithm \( \mathcal{A} \):] Given a sample \( \mathcal{S} \), the algorithm returns the tightest interval I' = \(I_{s}\) containing all points labeled with 1 in sample S.
  \item[Error intervals:] We define error regions as the intervals that lie between [a,b] but outside I'.
  \item[Proof of PAC learnability]:\\
    Let I be the target concept --- [a,b].\\
    Let \( \epsilon \) > 0 be the error.\\
    Let m be the number of samples in S.\\
    Let I\(_{s}\) be the tightest interval formed by the the points labelled 1 from sample S.\\
    Let Pr\([I_{s}]\) denote the probability mass of the interval defined by I\(_{s}\).\\
    \centerline{Pr\([I_{s}]\) > \( \epsilon \)}

    Lets assume that the interval I\(_{s}\) (denoted by [a',b']) has error \(\epsilon\).  The error can be found in intervals [a,a') and (b',b] denoted by \(I_{1} and I_{2} \) respectively.  If we assume that the error is equally distributed across the two regions, we can denote the error to be \(\epsilon/2\) for each of \(I_{1}\) and \(I_{2}\).  Each point in the error region has a probability of (1-\(\frac{\epsilon}{2}\)).  So Probability of the error in sample being greater than \(\epsilon \) can be written as
    \begin{align*}
      \underset{S\sim\mathcal{D}^{m}}{\Pr}[R(I_{S}) > \epsilon] \le & \sum_{i=1}^{2} \underset{S\sim\mathcal{D}^{m}}{\Pr} [{I  \bigcap I_{i} = \emptyset }]\\
      \le & {2( 1 - \epsilon/2)}^{m} \\
      \le & 2\mathrm{e}^{-m\epsilon/2}\\
    \end{align*}
    Equating the RHS to \( \delta \) gives us the sample complexity.
    \begin{align*}
      2\mathrm{e}^{-m\epsilon/2} = & \delta\\
      \frac{2}{\delta} = & \mathrm{e}^{m\epsilon/2} \\
      \log{\frac{2}{\delta}} = & \frac{m\epsilon}{2} \\
      m = & \frac{2}{\epsilon} \log{\frac{2}{\delta}}
    \end{align*}

\end{description}
\newpage
\subsection*{2}
\begin{description}
  \item[Algorithm \( \mathcal{A} \):]  Given a sample \( \mathcal{S}\) for target concept C containing p closed intervals,
    \begin{enumerate}
      \item If there are p separate sequence of positively labeled points in the training data, then return the union of the p tightest intervals containing the positive points.
      \item Otherwise, return (p - i) tightest intervsals, each containing a sequence of positie labels separfated by i negative labels.  i can take the values 0 to (p -1).
    \end{enumerate}
  \item[Error intervals:] Let \( [a_{1}, b_{1}] \cup [a_{2}, b_{2}] \cup ... [a_{p}, b_{p}] \) be the target concept C.  Let \( \epsilon > 0\). We can assume that \(\Pr[a_{i}, b_{i}] > \epsilon/(p + 1) \).  The actual error on the training set is gap between the target concept \( [a_{i}, b_{i}]\) and the learned concept \( [a'_{i}, b'_{i}] \).  Each of the error regions \( r_{i}\) can occur with a probability \( \epsilon/2(p + 1) \). (Factor of 2 for \( [a_{i}, a'_{i}] and [b'_{i}, b{i}] \).
  \item[Proof of PAC learnability]:\\
    If \( \mathrm{error(h_{S})} > \epsilon\), then either the union of the intervals misses one of the regions \( r_{i} \) or \( \Pr[b_{i}, a_{i+1}] > \epsilon/(p+1)\).  Thus by union bound, we have
    \begin{align*}
      \Pr[\mathrm{error(h_{S})}] \le & \Pr[\exists{i} \in [1, p]: h_{S} \; \mathrm{misses} \; r_{i}] + e^{-m\epsilon/(p+1)}\\
      \le & 2p(1-\frac{\epsilon}{2(p+1)})^{m} + e^{-m\epsilon/(p+1)}\\
      \le & 2pe^{-m\epsilon/{2(p+1)}} + e^{-m\epsilon/(p+1)}\\
      \le & (2p+1)e^{-m\epsilon/2(p+1)}
    \end{align*}

    Setting \( \delta \) = 0 and solving the RHS will give us a bound for m.

    \begin{align}
      (2p+1)e^{-m\epsilon/2(p+1)} = & \delta\\
      \frac{2p+1}{\delta} = & e^{m\epsilon/2(p+1)}\\
      \log{\frac{2p+1}{\delta}} = & \frac{m\epsilon}{2(p+1)}\\
      m = & \frac{2(p+1)}{\epsilon} \log{\frac{2p+1}{\delta}}
    \end{align}
    Therefore, for probability of error to be less than \(\epsilon\) m should be greater than the value obtained in (4) (Sample complexity).
  \item[When p is 2]:\\
    Substituting p = 2 in the equation (4) above gives us \(m \ge \frac{6}{\epsilon} \log{\frac{5}{\delta}}\) as the sample complexity.
  \item[Time complexity]:\\
    The time complexity is O(p).
\end{description}

\newpage
\section*{B. Rademacher complexity, growth function}
\subsection*{1}

The upper bound on the growth function can be found by finding out the number of dichotomies possible using the given hypothesis set for a given m.  For a given value of \( \theta \), the functions in H can classify either the points to the left of \(\theta\) as `\( + \)' (\( x \mapsto 1_{x \le \theta} : \theta \in \mathbb{R} \)) or as `\( - \)' (\( x \mapsto 1_{x \ge \theta} : \theta \in \mathbb{R} \)).  Similarly H can classify points to the right of \( \theta \) as `\( + \)' or `\( - \)'.

If we have a sample of size \(m\), we have a total of \((m+1)\) ways of classifying the sample.  Each classification can be done in two ways (as seen in the previous paragraph).  Hence we have a total of 2(m+1) ways of classification.  Of these classifications, two classification will be repeated (the one at the extremeties).  So we have to subtract 2 from the total ways of classification.  Hence total ways of classification is \(  2(m+1) - 2 = 2m\).
\begin{align*}
  \prod_{H}(m) = & 2m
\end{align*}

\subsubsection*{Upper bound on \( \mathfrak{R}_{m}(H)\)}

The upper bound is given by 
\begin{align*}
  \mathfrak{R}_{m}(H) \le & \sqrt{\frac{2log\prod_{H}(m)}{m}}\\
  \le & \sqrt{\frac{2log(2m)}{m}}
\end{align*}

\newpage
\subsection*{2}

\begin{align}
  \hat{\mathfrak{R}_{S}}(H) = & \frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H} \sum_{i = 1}^{m} \sigma_{i}h_{1}(x_{i})h_{2}(x_{i}) ] \\
  = & \frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H} \sum_{i = 1}^{m} \sigma_{i}( \mathrm{max}(0, h_{1}(x_{i}) + h_{2}(x_{i}) - 1 ) ] \\
  \le &\frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H} \sum_{i = 1}^{m} \sigma_{i}( h_{1}(x_{i}) + h_{2}(x_{i})) ] \\ 
  \le &\frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H} \sum_{i = 1}^{m} \sigma_{i}h_{1}(x_{i}) + \sigma_{i}h_{2}(x_{i}) ] \\ 
  \le &\frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H} \sum_{i = 1}^{m} \sigma_{i}h_{1}(x_{i}) + \sum_{i = 1}^{m} \sigma_{i}h_{2}(x_{i}) ] \\ 
  \le &\frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H_{1}} \sum_{i = 1}^{m} \sigma_{i}h_{1}(x_{i}) + \sup_{h \in H_{2}} \sum_{i = 1}^{m} \sigma_{i}h_{2}(x_{i}) ] \\ 
  \le &\frac{1}{m} \underset{\sigma}{E} [\sup_{h \in H_{1}} \sum_{i = 1}^{m} \sigma_{i}h_{1}(x_{i})] + \frac{1}{m} \underset{\sigma}{E}[ \sup_{h \in H_{2}} \sum_{i = 1}^{m} \sigma_{i}h_{2}(x_{i}) ] \\ 
  \le & \hat{\mathfrak{R}_{S}}(H_{1}) + \hat{\mathfrak{R}_{S}}(H_{2})
\end{align}

The equations and its explanations are as follows:

6 --- by rewriting \( h_{1}(x_{i})h_{2}(x_{i})\) in 1-lipshitz function form.  The lipshitz form is valid as \( h_{1}(x_{i}\) and \( h_{2}(x_{i}\) take values {0,1}.

7 --- due to Talagrand's lemma

10 --- Since \(sup(f+g) \le sup(f) + sup(g) \)

8, 9, 11, 12 --- Expanding the equation and replacing with Rademacher complexity terms.

\end{document}
