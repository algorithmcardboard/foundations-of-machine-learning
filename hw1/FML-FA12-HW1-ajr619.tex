\documentclass{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage[T1]{fontenc}

\begin{document}

\title{Foundations of Machine Learning --- Homework Assignment 1}
\date{October 11, 2015}
\author{Anirudhan J Rajagopalan\\ N18824115\\ ajr619}

\maketitle

\newpage

\section*{A. PAC Learning}
\subsection*{1}
\begin{description}
  \item[Algorithm \( \mathcal{A} \):] Given a sample \( \mathcal{S} \), the algorithm returns the tightest interval I' = \(I_{s}\) containing all points labeled with 1 in sample S.
  \item[Error intervals:] We define error regions as the intervals that lie between [a,b] but outside I'.
  \item[Proof of PAC learnability]:\\
    Let I be the target concept --- [a,b].\\
    Let \( \epsilon \) > 0 be the error.\\
    Let m be the number of samples in S.\\
    Let I\(_{s}\) be the tightest interval formed by the the points labelled 1 from sample S.\\
    Let Pr\([I_{s}]\) denote the probability mass of the interval defined by I\(_{s}\).\\
    \centerline{Pr\([I_{s}]\) > \( \epsilon \)}

    Lets assume that the interval I\(_{s}\) (denoted by [a',b']) has error \(\epsilon\).  The error can be found in intervals [a,a') and (b',b] denoted by \(I_{1} and I_{2} \) respectively.  If we assume that the error is equally distributed across the two regions, we can denote the error to be \(\epsilon/2\) for each of \(I_{1}\) and \(I_{2}\).  Each point in the error region has a probability of (1-\(\frac{\epsilon}{2}\)).  So Probability of the error in sample being greater than \(\epsilon \) can be written as
    \begin{align*}
      \underset{S\sim\mathcal{D}^{m}}{\Pr}[R(I_{S}) > \epsilon] \le & \sum_{i=1}^{2} \underset{S\sim\mathcal{D}^{m}}{\Pr} [{I  \bigcap I_{i} = \emptyset }]\\
      \le & {2( 1 - \epsilon/2)}^{m} \\
      \le & 2\mathrm{e}^{-m\epsilon/2}\\
    \end{align*}
    Equating the RHS to \( \delta \) gives us the sample complexity.
    \begin{align*}
      2\mathrm{e}^{-m\epsilon/2} = & \delta\\
      \frac{2}{\delta} = & \mathrm{e}^{m\epsilon/2} \\
      \log{\frac{2}{\delta}} = & \frac{m\epsilon}{2} \\
      m = & \frac{2}{\epsilon} \log{\frac{2}{\delta}}
    \end{align*}

\end{description}
\newpage
\subsection*{2}
\begin{description}
  \item[Algorithm \( \mathcal{A} \):]  Given a sample \( \mathcal{S}\) for target concept C containing p closed intervals,
    \begin{enumerate}
      \item If there are p separate sequence of positively labeled points in the training data, then return the union of the p tightest intervals containing the positive points.
      \item Otherwise, return (p - i) tightest intervsals, each containing a sequence of positie labels separfated by i negative labels.  i can take the values 0 to (p -1).
    \end{enumerate}
  \item[Error intervals:] Let \( [a_{1}, b_{1}] \cup [a_{2}, b_{2}] \cup ... [a_{p}, b_{p}] \) be the target concept C.  Let \( \epsilon > 0\). We can assume that \(\Pr[a_{i}, b_{i}] > \epsilon/(k + 1) \).  The actual error on the training set is gap between the target concept \( [a_{i}, b_{i}]\) and the learned concept \( [a'_{i}, b'_{i}] \).  Each of the error regions \( r_{i}\) can occur with a probability \( \epsilon/2(k + 1) \). (Factor of 2 for \( [a_{i}, a'_{i}] and [b'_{i}, b{i}] \).
  \item[Proof of PAC learnability]:\\
    If \( \mathrm{error(h_{S})} > \epsilon\), then either the union of the intervals misses one of the regions \( r_{i} \) or \( \Pr[b_{i}, a_{i+1}] > \epsilon/(k+1)\).  Thus by union bound, we have
    \begin{align*}
      \Pr[\mathrm{error(h_{S})}] \le & \Pr[\exists{i} \in [1, p]: h_{S} \; \mathrm{misses} \; r_{i}] + e^{-m\epsilon/(k+1)}\\
      \le & 2k(1-\frac{\epsilon}{2(k+1)})^{m} + e^{-m\epsilon/(k+1)}\\
      \le & 2ke^{-m\epsilon/{2(k+1)}} + e^{-m\epsilon/(k+1)}\\
      \le & (2k+1)e^{-m\epsilon/2(k+1)}
    \end{align*}

    Setting \( \delta \) = 0 and solving the RHS will give us a bound for m.

    \begin{align}
      (2k+1)e^{-m\epsilon/2(k+1)} = & \delta\\
      \frac{2k+1}{\delta} = & e^{m\epsilon/2(k+1)}\\
      \log{\frac{2k+1}{\delta}} = & \frac{m\epsilon}{2(k+1)}\\
      m = & \frac{2(k+1)}{\epsilon} \log{\frac{2k+1}{\delta}}
    \end{align}
    Therefore, for probability of error to be less than \(\epsilon\) m should be greater than the value obtained in (4).
  \item[When p is 2]:\\
    Substituting k = 2 in the equation (4) above gives us \(m \ge \frac{6}{\epsilon} \log{\frac{5}{\delta}}\)

\end{description}

\end{document}
