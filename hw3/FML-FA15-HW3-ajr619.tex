\documentclass{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\pagestyle{fancy}
\lhead{Anirudhan J Rajagopalan --- ajr619 --- N18824115}

\begin{document}

\title{Foundations of Machine Learning --- Homework Assignment 3}
\date{December 09, 2015}
\author{Anirudhan J Rajagopalan\\ N18824115\\ ajr619}

\maketitle

\newpage

\section*{A. Boosting-type Algorithm}
\subsection*{1. Bound of \( 1_{u \le 0} \) Proof of convexity and differentiability}
\begin{description}
  \item{Given: } \( \phi_{p}(u) = \max( {(1 + u)}^{p}, 0) \)
  \item{To prove:} 1. Function \( \phi_{p}(u) \) is convex and differentiable \\ And 2. \( \forall u \in \mathbb{R} \textrm{ and } p > 1,  1_{u \le 0} \le \phi_{p}(-u) \)
  \item{Proof of \( \forall u \in \mathbb{R} \textrm{ and } p > 1,  1_{u \le 0} \le \phi_{p}(-u) \)}
    There are three cases here:
    
    When \( u = 0\)
    \begin{align*}
        1_{u \le 0} = &1 \\
        \phi_{p}(-u) = & \max( {(1 - 0)}^{p}, 0) = 1 = 1_{u \le 0}
    \end{align*}

    When \( u < 0\)
    \begin{align*}
        1_{u \le 0} = &1 \\
        \phi_{p}(-u) = & \max( {(1 - u)}^{p}, 0)  \\
        =& \max( {(1 + u)}^{p}, 0)  \textrm{Since u is negative, -u is positive} \\
        > & 1 \\
        > & 1_{u \le 0}
    \end{align*}

    When \( u > 0 \)
    \begin{align*}
        1_{u \le 0} = & 0 \\
        \phi_{p}(-u) = & \max( {(1 - u)}^{p}, 0)  \\
        = & 
        \begin{cases}
          0, & \textrm{if p is odd} \\
          {(1-u)}^p, & \textrm{if p is positive}
        \end{cases} \\
        \ge & 0 \;\;\;\; \forall \;\;\;\; \mathbb{R}
        \ge 1_{u \le 0}
    \end{align*}

    Hence proved
  \item{Proof of Convexity and Differentiability:}
    The function can be written as a piecewise function based on the value of p:

    When p is even (2 and more)
    \begin{align*}
        \phi_{p}\left(x\right) = & {(1+u)}^p
    \end{align*}


    When p is odd (1 and more) the 
    \begin{align*}
        \phi_{p}\left(x\right) = 
        \begin{cases}
          0, & \textrm{if } u \le -1 \\
          {(1+u)}^p, & otherwise
        \end{cases}
    \end{align*}
\end{description}

\newpage

\section*{C. Randomized Halving}
\subsection*{1. Psuedo code}

\begin{algorithm}
  \caption{Randomized Halving}\label{euclid}
  \begin{algorithmic}[1]
    \State{} $ H_{1} \gets H$
    \For{t $ \gets{} $ 1 to T} 
    \State{} $RECEIVE(x_{t})$
    \State{} \( r_{t} \gets{} \frac{\sum_{i:y_{t,i} = 1} 1}{\mid H_{t} \mid}  \)
    \State{} \( p_{t} \gets{} 1 \)
    \If{\( r_{t} \le \frac{3}{4} \)} 
    \State{} \( p_{t} \gets{} [\frac{1}{2} \log_2{\frac{1}{1- r_{t}}} ] \)
    \EndIf{}
    \State{} \( \hat{y_{t}} \gets{} GetRandomNumberWithProbability([1, 0], [p_{t}, 1-p_{t}]) \)
    \State{} $RECEIVE(y_{t})$
    \If{\( \hat{y_{t}} \neq y_{t} \)} 
    \State{} \( H_{t+1}  \gets{} \{ c \in H_{t}: c(x_{t}) = y_{t} \}\)
    \EndIf{}
    \EndFor{}
    \Return{} $H_{T+1} $
  \end{algorithmic}
\end{algorithm}

\subsection*{2. Prove \( \forall t \geq 1, E[\mu] \le \frac{\phi_{t} - \phi_{t+1}}{2} \)}
\begin{description}
  \item{Given:} Potential function: \( \phi_{t} = \log_2{\mid H_{t} \mid} \) and \( \mu_{t} = 1_{y_{t} \neq \hat{y_{t}} } \)
  \item{Proof:} \\
    We are only considering the case when the predicted value \( \hat{y_{t}} \) is not equal to the received value \( y_{t} \).
    The value of expectation can be written as 
    \begin{align*}
      E[\mu_{t}] =& p_{t}*1 + (1 - p_{t})*0 \\
      =& p_{t}*1
    \end{align*}

    The probability of predicting 1 by the randomized algorithm is the probability of making a mistake since we are only considering the cases in which we make mistakes \(  ( \mu_{t} = 1_{y_{t} \neq \hat{y_{t}}})\)

    Therefore, 
    \begin{align*}
      E[\mu_{t}] =& p_{t}\\
      =& [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}} ] 1_{r_{t} \le \frac{3}{4}} + 1_{r_{t} > \frac{3}{4}} \\
      \le & [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}}]
    \end{align*}
    Since, the Expectation will be 1 when \( r_{t} > \frac{3}{4} \) which corresponds to the maximum expectation here, we can upper bound the expectation by using \( r_{t} \le \frac{3}{4} \) as \( [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}} ] \) equals 1 when \( r_{t} = \frac{3}{4} \)

    Let \( E_{1}, E_{0}, E_{t} \) denote the number of experts predicting 1, 0 and the total number of experts in a round t.

    \begin{align*}
      E[\mu_{t}] \le & [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}}] \\
      = & [ \frac{\log_2{\frac{1}{1 - r_{t}}}}{2}] \\
      = & [ \frac{\log_2{\frac{1}{1 - r_{t}}}}{2}] \\
      = & [ \frac{\log_2{\frac{1}{1 - \frac{\mid E_{1} \mid}{\mid H_{t} \mid}}}}{2}] \\
      = & [ \frac{\log_2{\frac{\mid H_{t} \mid}{\mid H_{t} \mid - \mid E_{1} \mid}}}{2}] \\
      = & [ \frac{\log_2{\frac{\mid H_{t} \mid}{\mid E_{0} \mid}}}{2}] \\
      = & [ \frac{\log_2{\mid H_{t} \mid} - \log_2{\mid E_{0} \mid}}{2}] \\
      = & [ \frac{\log_2{\mid H_{t} \mid} - \log_2{\mid H_{t+1} \mid}}{2}] \\
      = & [ \frac{\phi_{t} - \phi_{t+1}}{2}] \\
      \therefore E[\mu_{t}] \le &  \frac{\phi_{t} - \phi_{t+1}}{2}
    \end{align*}
\end{description}

\subsection*{3. Expected number of mistakes.}
\begin{description}
  \item{Given:} N be the total number of experts at the beginnning of the iterations (denoted by \( H_{1} \)). Since we are considering a relizable scenario at the end of the algorithm the number of experts should be atleast one.  So \( H_{T} = 1 \). 
  \item{To Prove:} The expected number of mistakes made by Randomized Halving is at most \( \frac{1}{2}\log_2{N} \)
  \item{Proof:} \\
    Lets consider the total expectation of mistakes of the Randomized Halving algorithm over T iterations.
    \begin{align*}
      E[\mu_{T}] \le & \sum_{t=1}^{T} \frac{\phi_{t} - \phi_{t+1}}{2} \\
      \le & \frac{(\phi_{H_{1}} - \phi_{H_{2}}) + (\phi_{H_{2}} - \phi_{H_{3}})  \cdots (\phi_{H_{T-2}} - \phi_{H_{T-1}}) + (\phi_{H_{T-1}} - \phi_{H_{T}}) \ }{2} \\
      \le & \frac{\phi_{H_{1}} - \phi_{H_{T}}}{2} \\
      \le & \frac{\phi_{N} - \phi_{T}}{2} \\
      \le & \frac{\phi_{N}}{2} \\
      \le & \frac{1}{2}\log_2{N}
    \end{align*}
    Hence proved.

    Here \( \phi_{T} = 0 \) because the number of experts at line T is 1.  Therefore, \( \log \) of T will be zero.
\end{description}

\subsection*{4. [Bonus Question]}
As we have seen in the previous answer, the mistakes made by the randomized algorithm is bounded by \( \frac{1}{2}\log_2{N} \).  This upper bound is dependent only on the number of initial experts, N.  Therefore any randomized algorithm that is dependent on the opinion of the experts to generate its predictions will have similar upper bound of \( \lfloor \frac{1}{2}\log_2{N} \rfloor \).  The floor function is used as mistakes are natural numbers.

\end{document}
