\documentclass{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\pagestyle{fancy}
\lhead{Anirudhan J Rajagopalan --- ajr619 --- N18824115}

\begin{document}

\title{Foundations of Machine Learning --- Homework Assignment 3}
\date{December 09, 2015}
\author{Anirudhan J Rajagopalan\\ N18824115\\ ajr619}

\maketitle

\newpage

\section*{A. Boosting-type Algorithm}
\subsection*{1. Bound of \( 1_{u \le 0} \) Proof of convexity and differentiability}
\begin{description}
  \item{Given: } \( \phi_{p}(u) = \max( {(1 + u)}^{p}, 0) \)
  \item{To prove:} 1. Function \( \phi_{p}(u) \) is convex and differentiable \\ And 2. \( \forall u \in \mathbb{R} \textrm{ and } p > 1,  1_{u \le 0} \le \phi_{p}(-u) \)
  \item{Proof of \( \forall u \in \mathbb{R} \textrm{ and } p > 1,  1_{u \le 0} \le \phi_{p}(-u) \)}
    There are three cases here:
    
    When \( u = 0\)
    \begin{align*}
        1_{u \le 0} = &1 \\
        \phi_{p}(-u) = & \max( {(1 - 0)}^{p}, 0) = 1 = 1_{u \le 0}
    \end{align*}

    When \( u < 0\)
    \begin{align*}
        1_{u \le 0} = &1 \\
        \phi_{p}(-u) = & \max( {(1 - u)}^{p}, 0)  \\
        =& \max( {(1 + u)}^{p}, 0)  \textrm{Since u is negative, -u is positive} \\
        > & 1 \\
        > & 1_{u \le 0}
    \end{align*}

    When \( u > 0 \)
    \begin{align*}
        1_{u \le 0} = & 0 \\
        \phi_{p}(-u) = & \max( {(1 - u)}^{p}, 0)  \\
        = & 
        \begin{cases}
          0, & \textrm{if p is odd} \\
          {(1-u)}^p, & \textrm{if p is positive}
        \end{cases} \\
        \ge & 0 \;\;\;\; \forall \;\;\;\; \mathbb{R}
        \ge 1_{u \le 0}
    \end{align*}

    Hence proved
  \item{Proof of Convexity and Differentiability:}
    The function can be written as a piecewise function based on the value of p:

    When p is even (2 and higher value even numbers)
    \begin{align*}
        \phi_{p}(u) = & {(1+u)}^p \\
    \end{align*}

    Since p > 1, the function is differentiable.

    \begin{align*}
        \phi^{'}_{p}(u) = & p{(1+u)}^{p - 1} \\
        \phi^{''}_{p}(u) = & {(p - 1)}p{(1+u)}^{p - 2} \\
        \because p > 1, \phi^{''}_{p}(u) > & 0
    \end{align*}

    Since the double derivative is greater than zero, the function is convex and differentiable.


    When p is odd (1 and higher value odd numbers) the function can be defined using the piecewise function
    \begin{align*}
        \phi_{p}\left(x\right) = 
        \begin{cases}
          0, & \textrm{if } u \le -1 \\
          {(1+u)}^p, & otherwise
        \end{cases}
    \end{align*}

    The function is piecewise continuous and differentiable. We have to check that the function is differentiable at u = -1 to show that the function is differentiable and continuous.  We should also check the double derivative to show that the function is convex.

    The first derivative can be found using limits.

    \begin{align*}
      \lim_{h \to 0+} \frac{\phi_{p}(-1 + h) - \phi_{p}(-1)}{h}=&  \lim_{h \to 0+} \frac{{(1 + (-1 + h))}^{p} - {(1 -1)}^{p}}{h} \\
      = & \lim_{h \to 0+} \frac{h^{p} - 0}{h} \\
      = & \lim_{h \to 0+} h^{p - 1} \\
      = & 0
    \end{align*}

    Also 
    
    \begin{align*}
      \lim_{h \to 0-} \frac{\phi_{p}(-1 + h) - \phi_{p}(-1)}{h}=&  \lim_{h \to 0+} \frac{{(1 + (-1 + h))}^{p} - {(1 -1)}^{p}}{h} \\
      = & \lim_{h \to 0-} \frac{h^{p} - 0}{h} \\
      = & \lim_{h \to 0-} h^{p - 1} \\
      = & 0
    \end{align*}

    Since both the limit values are equal, the function is differentiable. Therefore:
    \begin{align*}
        \phi^{'}_{p}\left(x\right) = 
        \begin{cases}
          0, & \textrm{if } u \le -1 \\
          p{(1+u)}^{p-1}, & otherwise
        \end{cases}
    \end{align*}

    For showing the function is convex, we find the double differeniable using limits for \( \phi^{'}_{p} \) and show that it is non negative

    \begin{align*}
      \lim_{h \to 0+} \frac{\phi^{'}_{p}(-1+h) - \phi^{'}_{p}(-1) }{h} = & \lim_{h \to 0+} \frac{p{(1 + (-1 + h))}^{p-1} - 0}{h} \\
      = & \lim_{h \to 0+} \frac{p{(h)}^{p-1}}{h} \\
      = & \lim_{h \to 0+} p{(h)}^{p-2} \\
      = 0
    \end{align*}

    \begin{align*}
      \lim_{h \to 0-} \frac{\phi^{'}_{p}(-1+h) - \phi^{'}_{p}(-1) }{h} = & \lim_{h \to 0-} \frac{p{(1 + (-1 + h))}^{p-1} - 0}{h} \\
      = & \lim_{h \to 0-} \frac{p{(h)}^{p-1}}{h} \\
      = & \lim_{h \to 0-} p{(h)}^{p-2} \\
      = 0
    \end{align*}

    Since the left and right derivatives are equal, the function is \( \phi^{'}_{p} \) is differentiable.  Which implies that the original function \( \phi_{p}\) is double differentiable.

    \begin{align*}
        \phi^{''}_{p}\left(x\right) = 
        \begin{cases}
          0, & \textrm{if } u \le -1 \\
          (p-1)p{(1+u)}^{p-2}, & otherwise
        \end{cases}
    \end{align*}
    
    Since in both cases the double differentiable is non negative, the function is convex and differentiable.

\end{description}

\subsection*{2. Derive boosting type algorithm}
We can build an algorithm similar to adaboost using coordinate descent.  The algorithm and the explanation are given below.
\begin{algorithm}
  \caption{Boosting type algorithm}
  \begin{algorithmic}[1]
    \State{} \( S = ( (x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{m}, y_{m})) \)
    \For{i $\gets$ 1 to m }
    \State{} $ D_{1}(i) \gets{}  \frac{1}{m}$
    \EndFor{}
    \For{i $\gets$ 1 to T }
    \State{} $h_{t} \gets{} $ base classifier in H with small error $ \epsilon_{t} = \Pr_{D_{t}}[h_{t}(x_{i}) \neq y_{i}] $
    \State{} $\alpha_{t} \gets{} \eta$ obtained by searching non-linear equation of $ \eta $
    \State{} $f_{t} \gets f_{t-1} + \alpha_{t}h_{t} $
    \EndFor{}
    \For{i $\gets$ 1 to m }
    \State{} $ D_{t}(i) \gets{}  \frac{1}{m} \frac{\phi^{'}_{p}(-y_{i}f_{t}(x_{i})) }{\sum_{i=1}^{m} \phi^{'}_{p}(-y_{i}f_{t}(x_{i}))}$
    \EndFor{}
    \Return{} \( h = sgn(f_{T}) \)
  \end{algorithmic}
\end{algorithm}

The objective function is defined as 
\begin{equation*}
  F(\alpha) = \sum_{i = 1}^{m} \phi_{p} (-y_{i}f_{T}(x_{i}))
\end{equation*}

Where
\begin{equation*}
  F_{T}(x_{i}) = \sum_{t=1}^{T} \alpha_{t}h_{t}(x_{i})
\end{equation*}
and the weighted distribution of the sample is 
\begin{equation*}
  D_{t}(i) \gets{}  \frac{1}{m} \frac{\phi^{'}_{p}(-y_{i}f_{t}(x_{i})) }{\sum_{i=1}^{m} \phi^{'}_{p}(-y_{i}f_{t}(x_{i}))}
\end{equation*}

The weighted distribution is initialized to \( \frac{1}{m} \) initially.


Let $ e_{t} $ denote the unit vector corresponding to the coordinate in $ \mathbb{R}^{n} $ and let $\alpha_{t - 1}$ denote the vector based on the (t-1) coefficients.

\begin{equation*}
  \alpha_{t-1} = 
  \begin{cases}
    {(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{t-1}, 0, 0, \ldots, 0)}^{T} & \mathrm{if (t-1) > 0}\\
    0 & otherwise
  \end{cases}
\end{equation*}

\subsubsection*{Direction}
Coordinate descent selects the direction $e_{t}$ that minimizes the directional derivative.
\begin{equation*}
  e_{t} = argmin_{t}{[\frac{dF(\alpha_{t-1} + \eta e_{t} )}{d\eta}]}_{\eta = 0}
\end{equation*}
where 
\begin{align*}
  F(\alpha_{t-1} + \eta e_{t} ) = & \sum_{i=1}^{m} \phi_{p} \left( -y_{i}\sum_{j=1}^{t-1} (\alpha_{j} + \eta e_{t})h_{j} \right) \\
  = & \sum_{i=1}^{m} \phi_{p} \left( -y_{i}\sum_{j=1}^{t-1} \alpha_{j}h_{j}(x_{i}) - \eta y_{i}h_{t}(x_{i}) \right)\\
\end{align*}

Also,
\begin{align*}
  \frac{dF(\alpha_{t-1} + \eta e_{t} )}{d\eta} = & -\sum_{i=1}^{m} y_{i}h_{t}(x_{i}).\phi^{'}_{p} \left( -y_{i}\sum_{j=1}^{t-1} \alpha_{j}h_{j}(x_{i}) \right) \\
  = & \sum_{i=1}^{m} y_{i}h_{t}(x_{i}).D(i).\left(m\sum_{k=1}^{m} \phi^{'}_{p}(-y_{k}f_{t}(x_{k})) \right) \\
  = & -\left(\sum_{y_{i}h_{t} = -1}D(i) + \sum_{y_{i}h_{t} = 1} D(i) \right) \left(m \sum_{k=1}^{m} \phi^{'}_{p}(-y_{k}f_{t}(x_{k}))\right) \\
  = & -\left( (1 - \epsilon_{t} - \epsilon_{t}) \right) \left( m\sum_{i = 1}^{m}\phi^{'}_{p} (-y_{i}f_{t}(x_{i})) \right) \\
  = & (2\epsilon_{t} - 1)\left( m\sum_{i = 1}^{m}\phi^{'}_{p} (-y_{i}f_{t}(x_{i})) \right) \\
  \varpropto & (2\epsilon_{t} - 1)
\end{align*}

Because of the above equation, it can be seen that the hypothesis chosen by the algorithm minimizes the mis-classification error.

\subsubsection*{Step}
We can obtain the step by minimizing the derivative of dF for $\eta$
\begin{align*}
  1 =& 2
\end{align*}
\subsubsection*{Generalization Bound}
A family of functions (H) taking values in {+1, -1} with VC-dimension d for any $ \delta > 0 $ with probability atleast $ 1 - \delta $, it holds for any $h \in conv(H)$:
\begin{align*}
  R(h) \le & \hat{R_{p}(h)} + \frac{2}{p} \sqrt{\frac{2d\log{\frac{em}{d}}}{m}} + \sqrt{\frac{\log{\frac{1}{\delta}}}{2m}}
\end{align*}

The above expression is true for all ensemble methods (Refer Corollary 6.2, page 133 of Foundations of Machine Learning, Mehryar Mohri et al.) it will be true for the algorithm described above.  Also we cannot have an emperical bound on $\hat{R}_p(h)$ in terms of misclassification error as there is not closed form expression of $\alpha_{t}$

\newpage

\section*{C. Randomized Halving}
\subsection*{1. Psuedo code}

\begin{algorithm}
  \caption{Randomized Halving}\label{euclid}
  \begin{algorithmic}[1]
    \State{} $ H_{1} \gets H$
    \For{t $ \gets{} $ 1 to T} 
    \State{} $RECEIVE(x_{t})$
    \State{} \( r_{t} \gets{} \frac{\sum_{i:y_{t,i} = 1} 1}{\mid H_{t} \mid}  \)
    \State{} \( p_{t} \gets{} 1 \)
    \If{\( r_{t} \le \frac{3}{4} \)} 
    \State{} \( p_{t} \gets{} [\frac{1}{2} \log_2{\frac{1}{1- r_{t}}} ] \)
    \EndIf{}
    \State{} \( \hat{y_{t}} \gets{} GetRandomNumberWithProbability([1, 0], [p_{t}, 1-p_{t}]) \)
    \State{} $RECEIVE(y_{t})$
    \If{\( \hat{y_{t}} \neq y_{t} \)} 
    \State{} \( H_{t+1}  \gets{} \{ c \in H_{t}: c(x_{t}) = y_{t} \}\)
    \EndIf{}
    \EndFor{}
    \Return{} $H_{T+1} $
  \end{algorithmic}
\end{algorithm}

\subsection*{2. Prove \( \forall t \geq 1, E[\mu] \le \frac{\phi_{t} - \phi_{t+1}}{2} \)}
\begin{description}
  \item{Given:} Potential function: \( \phi_{t} = \log_2{\mid H_{t} \mid} \) and \( \mu_{t} = 1_{y_{t} \neq \hat{y_{t}} } \)
  \item{Proof:} \\
    We are only considering the case when the predicted value \( \hat{y_{t}} \) is not equal to the received value \( y_{t} \).
    The value of expectation can be written as 
    \begin{align*}
      E[\mu_{t}] =& p_{t}*1 + (1 - p_{t})*0 \\
      =& p_{t}*1
    \end{align*}

    The probability of predicting 1 by the randomized algorithm is the probability of making a mistake since we are only considering the cases in which we make mistakes \(  ( \mu_{t} = 1_{y_{t} \neq \hat{y_{t}}})\)

    Therefore, 
    \begin{align*}
      E[\mu_{t}] =& p_{t}\\
      =& [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}} ] 1_{r_{t} \le \frac{3}{4}} + 1_{r_{t} > \frac{3}{4}} \\
      \le & [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}}]
    \end{align*}
    Since, the Expectation will be 1 when \( r_{t} > \frac{3}{4} \) which corresponds to the maximum expectation here, we can upper bound the expectation by using \( r_{t} \le \frac{3}{4} \) as \( [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}} ] \) equals 1 when \( r_{t} = \frac{3}{4} \)

    Let \( E_{1}, E_{0}, E_{t} \) denote the number of experts predicting 1, 0 and the total number of experts in a round t.

    \begin{align*}
      E[\mu_{t}] \le & [\frac{1}{2} \log_2{\frac{1}{1 - r_{t}}}] \\
      = & [ \frac{\log_2{\frac{1}{1 - r_{t}}}}{2}] \\
      = & [ \frac{\log_2{\frac{1}{1 - r_{t}}}}{2}] \\
      = & [ \frac{\log_2{\frac{1}{1 - \frac{\mid E_{1} \mid}{\mid H_{t} \mid}}}}{2}] \\
      = & [ \frac{\log_2{\frac{\mid H_{t} \mid}{\mid H_{t} \mid - \mid E_{1} \mid}}}{2}] \\
      = & [ \frac{\log_2{\frac{\mid H_{t} \mid}{\mid E_{0} \mid}}}{2}] \\
      = & [ \frac{\log_2{\mid H_{t} \mid} - \log_2{\mid E_{0} \mid}}{2}] \\
      = & [ \frac{\log_2{\mid H_{t} \mid} - \log_2{\mid H_{t+1} \mid}}{2}] \\
      = & [ \frac{\phi_{t} - \phi_{t+1}}{2}] \\
      \therefore E[\mu_{t}] \le &  \frac{\phi_{t} - \phi_{t+1}}{2}
    \end{align*}
\end{description}

\subsection*{3. Expected number of mistakes.}
\begin{description}
  \item{Given:} N be the total number of experts at the beginnning of the iterations (denoted by \( H_{1} \)). Since we are considering a relizable scenario at the end of the algorithm the number of experts should be atleast one.  So \( H_{T} = 1 \). 
  \item{To Prove:} The expected number of mistakes made by Randomized Halving is at most \( \frac{1}{2}\log_2{N} \)
  \item{Proof:} \\
    Lets consider the total expectation of mistakes of the Randomized Halving algorithm over T iterations.
    \begin{align*}
      E[\mu_{T}] \le & \sum_{t=1}^{T} \frac{\phi_{t} - \phi_{t+1}}{2} \\
      \le & \frac{(\phi_{H_{1}} - \phi_{H_{2}}) + (\phi_{H_{2}} - \phi_{H_{3}})  \cdots (\phi_{H_{T-2}} - \phi_{H_{T-1}}) + (\phi_{H_{T-1}} - \phi_{H_{T}}) \ }{2} \\
      \le & \frac{\phi_{H_{1}} - \phi_{H_{T}}}{2} \\
      \le & \frac{\phi_{N} - \phi_{T}}{2} \\
      \le & \frac{\phi_{N}}{2} \\
      \le & \frac{1}{2}\log_2{N}
    \end{align*}
    Hence proved.

    Here \( \phi_{T} = 0 \) because the number of experts at line T is 1.  Therefore, \( \log \) of T will be zero.
\end{description}

\subsection*{4. [Bonus Question]}
As we have seen in the previous answer, the mistakes made by the randomized algorithm is bounded by \( \frac{1}{2}\log_2{N} \).  This upper bound is dependent only on the number of initial experts, N.  Therefore any randomized algorithm that is dependent on the opinion of the experts to generate its predictions will have similar upper bound of \( \lfloor \frac{1}{2}\log_2{N} \rfloor \).  The floor function is used as mistakes are natural numbers.

\end{document}
